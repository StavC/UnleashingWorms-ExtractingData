{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-28T09:19:11.534828Z",
     "start_time": "2024-08-28T09:19:01.800357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from chromadb.config import Settings\n",
    "import chromadb\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Constants\n",
    "GEMINI_API_KEY = ''  # Insert your Gemini API key here\n",
    "\n",
    "# Detect device (CUDA if available, else CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Experiment settings\n",
    "CreateNewVectorStore=False \n",
    "k = 20\n",
    "dim=768 \n",
    "ModelName='Gte_base'\n",
    "SuffixDir='EnglishSuffix'\n",
    "#SuffixDir='RandomSuffix'\n",
    "CSVName='Sentences_To_Test.csv' #CSV file with the perturbed sentences\n",
    "Vectors_df=pd.read_csv(f'./EvalForPaper/{ModelName}/PerturbedEmbedding/{SuffixDir}/{CSVName}')\n",
    "\n",
    "FolderPerturbedVectorsName= str(ModelName) + str(dim) + SuffixDir\n",
    "SavePath=f'./results/GeminiResponse/{ModelName}/{FolderPerturbedVectorsName}'\n",
    "\n",
    "\n",
    "# Ensure the save directory exists\n",
    "if not os.path.exists(SavePath):\n",
    "    os.makedirs(SavePath)\n",
    "\n",
    "# Print CUDA availability status\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Embedding Models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4eef61b65c64a7e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Embedding Models\n",
    "\n",
    "class MiniLMEmbeddings:\n",
    "    \"\"\"Embeddings using the MiniLM model from HuggingFace.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cuda\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [self._embed(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self._embed(text)\n",
    "\n",
    "    def _embed(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        model_output = self.model(**inputs)\n",
    "        sentence_embeddings = self._mean_pooling(model_output, inputs['attention_mask'])\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        return sentence_embeddings.detach().cpu().numpy().flatten().tolist()\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "class NomicEmbeddings:\n",
    "    \"\"\"Embeddings using the Nomic model.\"\"\"\n",
    "    \n",
    "    def __init__(self, device=\"cuda\", embedding_dim=384):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = AutoModel.from_pretrained('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True).to(device)\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [self._embed(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self._embed(text)\n",
    "\n",
    "    def _embed(self, text):\n",
    "        encoded_input = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(self.device)\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "            embeddings = self._mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "            embeddings = F.layer_norm(embeddings, embeddings.size()[1:])\n",
    "            embeddings = embeddings[:, :self.embedding_dim]\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        return embeddings.cpu().numpy().flatten().tolist()\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "class GteSmallEmbeddings:\n",
    "    \"\"\"Embeddings using the GTE-Small model.\"\"\"\n",
    "    \n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\n",
    "        self.model = AutoModel.from_pretrained(\"thenlper/gte-small\").to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [self._embed(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self._embed(text)\n",
    "\n",
    "    def _embed(self, text):\n",
    "        encoded_input = self.tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output.last_hidden_state, encoded_input['attention_mask'])\n",
    "            sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        return sentence_embeddings.cpu().numpy().flatten().tolist()\n",
    "\n",
    "    def _mean_pooling(self, last_hidden_states, attention_mask):\n",
    "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "class GteBaseEmbeddings:\n",
    "    \"\"\"Embeddings using the GTE-Base model.\"\"\"\n",
    "    \n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-base\")\n",
    "        self.model = AutoModel.from_pretrained(\"thenlper/gte-base\").to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [self._embed(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self._embed(text)\n",
    "\n",
    "    def _embed(self, text):\n",
    "        encoded_input = self.tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output.last_hidden_state, encoded_input['attention_mask'])\n",
    "            sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        return sentence_embeddings.cpu().numpy().flatten().tolist()\n",
    "\n",
    "    def _mean_pooling(self, last_hidden_states, attention_mask):\n",
    "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "class GteLargeEmbeddings:\n",
    "    \"\"\"Embeddings using the GTE-Large model.\"\"\"\n",
    "    \n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-large\")\n",
    "        self.model = AutoModel.from_pretrained(\"thenlper/gte-large\").to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [self._embed(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self._embed(text)\n",
    "\n",
    "    def _embed(self, text):\n",
    "        encoded_input = self.tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output.last_hidden_state, encoded_input['attention_mask'])\n",
    "            sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        return sentence_embeddings.cpu().numpy().flatten().tolist()\n",
    "\n",
    "    def _mean_pooling(self, last_hidden_states, attention_mask):\n",
    "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "class MpnetEmbeddings:\n",
    "    \"\"\"Embeddings using the Mpnet model.\"\"\"\n",
    "    \n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "        self.model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2').to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [self._embed(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self._embed(text)\n",
    "\n",
    "    def _embed(self, text):\n",
    "        encoded_input = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "            sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        return sentence_embeddings.cpu().numpy().flatten().tolist()\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "274362dbccd079c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to load JSON data and ensure documents are unique\n",
    "def load_data(data_path, num_docs):\n",
    "    with open(data_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Randomly sample num_docs documents from the data\n",
    "    sampled_data = random.sample(list(enumerate(data)), min(num_docs, len(data)))\n",
    "    index = 0\n",
    "    documents = []\n",
    "    seen_texts = set()\n",
    "    for _, item in sampled_data:\n",
    "        doc_text = item['input']\n",
    "        if doc_text in seen_texts:\n",
    "            print('found a duplicate document, skipping...')\n",
    "            continue\n",
    "        seen_texts.add(doc_text)\n",
    "        doc_metadata = {\"DoctorAnswer\": item['output'], \"index\": index}\n",
    "        documents.append(Document(page_content=doc_text, metadata=doc_metadata))\n",
    "        index += 1\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "def load_vector_store(persist_directory, embedding_model, collection_name):\n",
    "    db = Chroma(persist_directory=persist_directory, collection_name=collection_name,\n",
    "                embedding_function=embedding_model, collection_metadata={\"hnsw:space\": \"cosine\"})\n",
    "    print(\"Vector store loaded.\")\n",
    "    return db\n",
    "\n",
    "\n",
    "def save_vector_store(documents, embedding_model, persist_directory, collection_name):\n",
    "    db = Chroma.from_documents(documents, embedding_model, collection_name=collection_name,\n",
    "                               persist_directory=persist_directory, collection_metadata={\"hnsw:space\": \"cosine\"})\n",
    "    print(\"Documents added and vector store persisted.\")\n",
    "    return db\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b01aefde009a3b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not os.path.exists(SavePath):\n",
    "    os.makedirs(SavePath)\n",
    "\n",
    "if ModelName == 'Nomic':\n",
    "    embedding_model = NomicEmbeddings(matryoshka_dim=dim)\n",
    "    Dim = dim\n",
    "elif ModelName == 'MiniLM':\n",
    "    embedding_model = MiniLMEmbeddings()\n",
    "    Dim = 384\n",
    "elif ModelName == 'Gte_small':\n",
    "    embedding_model = GteSmallEmbeddings()\n",
    "    Dim = 384\n",
    "elif ModelName == 'Gte_base':\n",
    "    embedding_model = GteBaseEmbeddings()\n",
    "    Dim = 768\n",
    "elif ModelName == 'Gte_large':\n",
    "    embedding_model = GteLargeEmbeddings()\n",
    "    Dim = 1024\n",
    "elif ModelName == 'Mpnet':\n",
    "    embedding_model = MpnetEmbeddings()\n",
    "    Dim = 768\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T09:19:14.677410Z",
     "start_time": "2024-08-28T09:19:11.534828Z"
    }
   },
   "id": "65cf1e0a281e631b",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if CreateNewVectorStore:\n",
    "    # Save the vector store (run this cell only once)\n",
    "    data_path = 'Data/HealthCareMagic/HealthCareMagic-100k.json'\n",
    "    num_docs = 1000\n",
    "    persist_directory = f\"./chroma_db{ModelName}{dim}\"\n",
    "    collection_name = 'v_db'\n",
    "\n",
    "    documents = load_data(data_path, num_docs)\n",
    "    db = save_vector_store(documents, embedding_model, persist_directory, collection_name)\n",
    "    print(f'created the vectorstore at {persist_directory}')\n",
    "else:\n",
    "    persist_directory = f\"./chroma_db{ModelName}{dim}\"\n",
    "    collection_name = 'v_db'\n",
    "    db = load_vector_store(persist_directory, embedding_model, collection_name)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T09:19:14.899806Z",
     "start_time": "2024-08-28T09:19:14.680601Z"
    }
   },
   "id": "a33683e4d9f41786",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to get response from Gemini\n",
    "def GetLLMResponse(db, query, k, gemini_api_key):\n",
    "\n",
    "    retrieved_docs = db.similarity_search_with_relevance_scores(query, k=k)\n",
    "\n",
    "\n",
    "    template = \"You are a medical QnA bot. You have to answer the following question: {query} \\n\\n _______________ \\n\\n Now use the following context to answer the request: {Context} \\n\\n Answer:\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    prompt = prompt.format(Context=retrieved_docs, query=query)\n",
    "\n",
    "    # Initialize and invoke the LLM\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.1, google_api_key=gemini_api_key,\n",
    "                                 safety_settings=None)\n",
    "    reply = llm.invoke(prompt)\n",
    "\n",
    "\n",
    "    return reply.content, retrieved_docs\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T09:19:14.905694Z",
     "start_time": "2024-08-28T09:19:14.902309Z"
    }
   },
   "id": "c1fb5498453ca3e0",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_or_fetch_indexes(text, db, query, k, gemini_api_key):\n",
    "    # Try extracting indexes using the regular expressions\n",
    "    indexes = extract_indexes(text)\n",
    "\n",
    "    # If the indexes list is empty, contact Gemini to get the indexes\n",
    "    if not indexes:\n",
    "        print('Extracting indexes using Gemini... as the extracted indexes list is empty')\n",
    "        # Prepare the query for Gemini to retrieve indexes\n",
    "        example_text = \"\"\"\n",
    "        For example, if the text is:\n",
    "        {\n",
    "            \"Content\": \"Some medical content...\",\n",
    "            \"Metadata\": {\n",
    "                \"DoctorAnswer\": \"Sample doctor answer...\",\n",
    "                \"index\": 123\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Content\": \"Another piece of content...\",\n",
    "            \"Metadata\": {\n",
    "                \"DoctorAnswer\": \"Another answer...\",\n",
    "                \"index\": 456\n",
    "            }\n",
    "        }\n",
    "        You should return: 123, 456\n",
    "        \"\"\"\n",
    "\n",
    "        gemini_query = f\"\"\"Please extract all index numbers from the following text and return them as a comma-separated list. \n",
    "        {example_text}\n",
    "        \n",
    "        Here is the text you need to analyze:\n",
    "        {text}\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the response from Gemini\n",
    "        llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.1, google_api_key=gemini_api_key,\n",
    "                                     safety_settings=None)\n",
    "        reply = llm.invoke(gemini_query)\n",
    "\n",
    "        # Parse the response to get the list of indexes\n",
    "        indexes = parse_gemini_indexes(reply.content)\n",
    "\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def extract_indexes(text):\n",
    "    # Regular expression to find all occurrences of 'index' or \"index\": followed by a number\n",
    "    index_pattern_single = r\"'index':\\s*(\\d+)\"\n",
    "    index_pattern_double = r'\"index\":\\s*(\\d+)'\n",
    "\n",
    "    # Find all matches in the string for both patterns\n",
    "    matches_single = re.findall(index_pattern_single, text)\n",
    "    matches_double = re.findall(index_pattern_double, text)\n",
    "\n",
    "    # Combine all matches\n",
    "    matches = matches_single + matches_double\n",
    "\n",
    "    # Convert matches to a list of integers\n",
    "    indexes = list(map(int, matches))\n",
    "\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def parse_gemini_indexes(gemini_response):\n",
    "    # Parse the Gemini response assuming it returns a comma-separated list of numbers\n",
    "    index_list = [int(i.strip()) for i in gemini_response.split(',') if i.strip().isdigit()]\n",
    "    return index_list\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T09:19:14.910359Z",
     "start_time": "2024-08-28T09:19:14.905694Z"
    }
   },
   "id": "12457bd72da39d6b",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run Gemini Response Experiments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4378b2adc57eacc7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Experiment_Df_backup = []\n",
    "exp1_df = []\n",
    "setOfIndexes = set()  \n",
    "setOfIndexesReplied = set()  \n",
    "\n",
    "for RowNumber, row in Vectors_df.iterrows():\n",
    "    print(f'Index: {RowNumber}')\n",
    "    NumberOfUniqueIndexesAdded = 0\n",
    "    IndexesRetrieved = []\n",
    "    IndexesAddedUnique = []\n",
    "    IndexesAddedUniqueCosineSimilarity = []\n",
    "    IndexesCosineSimilarity = []\n",
    "    IndexesReplied = []\n",
    "    IndexesRepliedCosineSimilarity = []\n",
    "    IndexesDuplicateReplied = []\n",
    "    IndexesDuplicatedCount = 0\n",
    "    HallucinatedIndexes = []\n",
    "\n",
    "    try:\n",
    "        query = row['Perturbed Sentence']\n",
    "    except:\n",
    "        query = row['perturbed_sentence']\n",
    "    reply, retrieved_docs = GetLLMResponse(db, query, k, GEMINI_API_KEY)\n",
    "    for doc in retrieved_docs:\n",
    "        if doc[0].metadata[\"index\"] not in setOfIndexes:\n",
    "            NumberOfUniqueIndexesAdded += 1\n",
    "            setOfIndexes.add(doc[0].metadata[\"index\"])\n",
    "            IndexesAddedUnique.append(doc[0].metadata[\"index\"])\n",
    "            IndexesAddedUniqueCosineSimilarity.append(doc[1])\n",
    "\n",
    "        IndexesRetrieved.append(doc[0].metadata[\"index\"])\n",
    "        IndexesCosineSimilarity.append(doc[1])\n",
    "\n",
    "    CurrentIndexListFromReply = extract_indexes(reply)\n",
    "\n",
    "    for CurrentDocindex in CurrentIndexListFromReply:\n",
    "        if CurrentDocindex not in IndexesRetrieved:  #Hallucinated Index\n",
    "            HallucinatedIndexes.append(CurrentDocindex)\n",
    "\n",
    "        else:  #Replied Index\n",
    "            if CurrentDocindex not in IndexesReplied:\n",
    "                IndexesReplied.append(CurrentDocindex)\n",
    "                DocCosine = IndexesCosineSimilarity[IndexesRetrieved.index(CurrentDocindex)]\n",
    "                IndexesRepliedCosineSimilarity.append(DocCosine)\n",
    "                setOfIndexesReplied.add(CurrentDocindex)\n",
    "            else:\n",
    "                IndexesDuplicateReplied.append(CurrentDocindex)\n",
    "                IndexesDuplicatedCount += 1\n",
    "\n",
    "    exp1_df.append([RowNumber, query, reply, IndexesRetrieved, IndexesCosineSimilarity, NumberOfUniqueIndexesAdded,\n",
    "                    IndexesAddedUnique, IndexesAddedUniqueCosineSimilarity, setOfIndexes.copy(), IndexesReplied,\n",
    "                    IndexesRepliedCosineSimilarity, IndexesDuplicateReplied, IndexesDuplicatedCount,\n",
    "                    HallucinatedIndexes, setOfIndexesReplied.copy()])\n",
    "\n",
    "\n",
    "    print('----------------------------------------')\n",
    "\n",
    "    if RowNumber == 0:\n",
    "        Experiment_Df_backup = pd.DataFrame([[RowNumber, query, reply, IndexesRetrieved, IndexesCosineSimilarity,\n",
    "                                              NumberOfUniqueIndexesAdded, IndexesAddedUnique,\n",
    "                                              IndexesAddedUniqueCosineSimilarity, setOfIndexes.copy(), IndexesReplied,\n",
    "                                              IndexesRepliedCosineSimilarity, IndexesDuplicateReplied,\n",
    "                                              IndexesDuplicatedCount, HallucinatedIndexes, setOfIndexesReplied.copy()]],\n",
    "                                            columns=['Index', 'Query', 'Reply', 'IndexesRetrieved',\n",
    "                                                     'IndexesCosineSimilarity', 'NumberOfUniqueIndexesAdded',\n",
    "                                                     'IndexesAddedUnique', 'IndexesAddedUniqueCosineSimilarity',\n",
    "                                                     'SetOfIndexes', 'IndexesReplied', 'IndexesRepliedCosineSimilarity',\n",
    "                                                     'IndexesDuplicateReplied', 'IndexesDuplicatedCount',\n",
    "                                                     'HallucinatedIndexes', 'SetOfIndexesReplied'])\n",
    "\n",
    "\n",
    "    else:\n",
    "        new_row = pd.DataFrame([[RowNumber, query, reply, IndexesRetrieved, IndexesCosineSimilarity,\n",
    "                                 NumberOfUniqueIndexesAdded, IndexesAddedUnique, IndexesAddedUniqueCosineSimilarity,\n",
    "                                 setOfIndexes.copy(), IndexesReplied, IndexesRepliedCosineSimilarity,\n",
    "                                 IndexesDuplicateReplied, IndexesDuplicatedCount, HallucinatedIndexes,\n",
    "                                 setOfIndexesReplied.copy()]],\n",
    "                               columns=['Index', 'Query', 'Reply', 'IndexesRetrieved', 'IndexesCosineSimilarity',\n",
    "                                        'NumberOfUniqueIndexesAdded', 'IndexesAddedUnique',\n",
    "                                        'IndexesAddedUniqueCosineSimilarity', 'SetOfIndexes', 'IndexesReplied',\n",
    "                                        'IndexesRepliedCosineSimilarity', 'IndexesDuplicateReplied',\n",
    "                                        'IndexesDuplicatedCount', 'HallucinatedIndexes', 'SetOfIndexesReplied'])\n",
    "\n",
    "        Experiment_Df_backup = pd.concat([Experiment_Df_backup, new_row], ignore_index=True)\n",
    "\n",
    "    Experiment_Df_backup.to_csv(f'{SavePath}/LLM_Reply_Backup.csv', index=False)\n",
    "\n",
    "exp1_df = pd.DataFrame(exp1_df, columns=['Index', 'Query', 'Reply', 'IndexesRetrieved', 'IndexesCosineSimilarity',\n",
    "                                         'NumberOfUniqueIndexesAdded', 'IndexesAddedUnique',\n",
    "                                         'IndexesAddedUniqueCosineSimilarity', 'SetOfIndexes', 'IndexesReplied',\n",
    "                                         'IndexesRepliedCosineSimilarity', 'IndexesDuplicateReplied',\n",
    "                                         'IndexesDuplicatedCount', 'HallucinatedIndexes', 'SetOfIndexesReplied'])\n",
    "exp1_df.to_csv(f'{SavePath}/LLM_Reply_Full.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a70b7c972c6e2bc2",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
